<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Gemma 3n Multimodal Tutor</title>
    <style>
        :root {
            --primary: #0ea5e9;
            --bg: #0f172a;
        }

        body {
            font-family: system-ui, sans-serif;
            background: var(--bg);
            color: white;
            margin: 0;
            display: flex;
            flex-direction: column;
            height: 100vh;
        }

        #chat {
            flex: 1;
            overflow-y: auto;
            padding: 20px;
            display: flex;
            flex-direction: column;
            gap: 15px;
        }

        .msg {
            padding: 12px 16px;
            border-radius: 12px;
            max-width: 80%;
            line-height: 1.5;
        }

        .user {
            align-self: flex-end;
            background: var(--primary);
        }

        .ai {
            align-self: flex-start;
            background: #1e293b;
            border: 1px solid #334155;
        }

        .controls {
            background: #1e293b;
            padding: 20px;
            display: flex;
            flex-direction: column;
            gap: 10px;
            border-top: 1px solid #334155;
        }

        .input-row {
            display: flex;
            gap: 10px;
        }

        input[type="text"] {
            flex: 1;
            padding: 12px;
            border-radius: 8px;
            border: 1px solid #334155;
            background: #0f172a;
            color: white;
        }

        button {
            padding: 10px 20px;
            border-radius: 8px;
            border: none;
            cursor: pointer;
            font-weight: bold;
        }

        #send-btn {
            background: var(--primary);
            color: white;
        }

        .media-btns {
            display: flex;
            gap: 10px;
            font-size: 0.8rem;
        }

        #loader {
            position: fixed;
            inset: 0;
            background: var(--bg);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            z-index: 10;
        }
    </style>
</head>

<body>

    <div id="loader">
        <h2>Initializing Gemma 3n E4B (Multimodal)</h2>
        <p id="status">Loading WASM & WebGPU layers...</p>
    </div>

    <div id="chat">
        <div class="msg ai">Hello! I can now "see" images and "hear" audio. Try uploading a screenshot of a math problem
            or recording a question!</div>
    </div>

    <div class="controls">
        <div class="media-btns">
            <input type="file" id="image-input" accept="image/*" style="display:none">
            <button onclick="document.getElementById('image-input').click()" style="background:#334155; color:white;">ðŸ“·
                Add Image</button>
            <button id="mic-btn" style="background:#334155; color:white;">ðŸŽ¤ Record Audio</button>
            <span id="media-status" style="color: #94a3b8; align-self: center;"></span>
        </div>
        <div class="input-row">
            <input type="text" id="user-input" placeholder="Ask a question about the media...">
            <button id="send-btn">Send</button>
        </div>
    </div>

    <script type="module">
        import { FilesetResolver, LlmInference } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai@latest';

        let llmInference;
        let selectedImage = null;
        let recordedAudio = null;
        let mediaRecorder = null;
        let audioChunks = [];

        async function toggleRecording() {
            const btn = document.getElementById('mic-btn');

            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                btn.innerText = "ðŸŽ¤ Record Audio";
                btn.style.backgroundColor = "#334155";
            } else {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    mediaRecorder = new MediaRecorder(stream);
                    audioChunks = [];

                    mediaRecorder.ondataavailable = (e) => {
                        if (e.data.size > 0) audioChunks.push(e.data);
                    };

                    mediaRecorder.onstop = () => {
                        const blob = new Blob(audioChunks, { type: mediaRecorder.mimeType });
                        const reader = new FileReader();
                        reader.onload = (e) => {
                            // Assign the Base64 Data URL to recordedAudio
                            recordedAudio = e.target.result;
                            document.getElementById('media-status').innerText = "Audio recorded";
                        };
                        reader.readAsDataURL(blob);
                    };

                    mediaRecorder.start();
                    btn.innerText = "Recording...";
                    btn.style.backgroundColor = "#ef4444";
                    document.getElementById('media-status').innerText = "Recording...";
                } catch (err) {
                    console.error("Error accessing microphone:", err);
                    alert("Could not access microphone. Ensure permissions are granted.");
                }
            }
        }

        document.getElementById('mic-btn').onclick = toggleRecording;

        async function init() {
            try {
                const genai = await FilesetResolver.forGenAiTasks('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai/wasm');

                const createOptions = (delegate) => ({
                    baseOptions: {
                        modelAssetPath: './gemma-3n-E2B-it-int4-Web.litertlm',
                        delegate: delegate
                    },
                    maxTokens: 1024,
                    temperature: 0.7,
                    // CRITICAL: Reserved memory for multimodal inputs
                    maxNumImages: 1,
                    supportAudio: true
                });

                document.getElementById('status').innerText = "Initializing with CPU...";

                llmInference = await LlmInference.createFromOptions(genai, createOptions('CPU'));

                document.getElementById('loader').style.display = 'none';
            } catch (e) {
                console.error(e);
                document.getElementById('status').innerText = "Error: " + e.message;
            }
        }

        // --- Media Handling ---
        document.getElementById('image-input').onchange = (e) => {
            const file = e.target.files[0];
            if (file) {
                const reader = new FileReader();
                reader.onload = (event) => {
                    selectedImage = new Image();
                    selectedImage.src = event.target.result;
                    document.getElementById('media-status').innerText = "Image attached";
                };
                reader.readAsDataURL(file);
            }
        };

        // --- Chat Logic ---
        async function handleChat() {
            const text = document.getElementById('user-input').value.trim();
            if ((!text && !selectedImage && !recordedAudio) || !llmInference) return;

            const chatContainer = document.getElementById('chat');
            const userMsg = document.createElement('div');
            userMsg.className = 'msg user';
            userMsg.innerText = text || (selectedImage ? "[Sent Image]" : "[Sent Audio]");
            chatContainer.appendChild(userMsg);

            // Multimodal Input Array
            const promptArray = ['<start_of_turn>user\n'];
            if (recordedAudio) promptArray.push({ audioSource: recordedAudio });
            if (selectedImage) promptArray.push({ imageSource: selectedImage });
            promptArray.push(`${text}<end_of_turn>\n<start_of_turn>model\n`);

            document.getElementById('user-input').value = '';
            const aiMsgDiv = document.createElement('div');
            aiMsgDiv.className = 'msg ai';
            chatContainer.appendChild(aiMsgDiv);

            let fullResponse = "";
            await llmInference.generateResponse(promptArray, (partial, done) => {
                fullResponse += partial;
                aiMsgDiv.innerText = fullResponse;
                chatContainer.scrollTop = chatContainer.scrollHeight;
                if (done) {
                    selectedImage = null;
                    recordedAudio = null;
                    document.getElementById('media-status').innerText = "";
                }
            });
        }

        document.getElementById('send-btn').onclick = handleChat;
        init();
    </script>
</body>

</html>